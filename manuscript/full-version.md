# What's in the full version

The full version discusses the conventional acceptance testing strategies used by many agile teams today. In Part 1, it describes generally accepted strategies, their motivations, pitfalls and techniques to maximise success. It talks about how testing influences design and how to avoid the common problem of too many tests and specification overload.

If you want tips applying conventional acceptance testing strategies, Part 1 can help you get started and avoid common mistakes. The [Introduction](#introduction) section from Part 1 is included in this sample.

In Part 2, the book discusses why some of these techniques are fundamentally floored and tries to pose some difficult questions. Has acceptance testings techniques become dogma? Can stories really have business "value"? How can we test value? Can we run thousands of acceptance tests quickly?

If you're interested in what's beyond the canon, Part 2 may get you thinking. Inspired by real world frustrations and lean principles, Part 2 questions the de facto agile stance on testing.

## Full table of contents

###Part 1 - The typical agile strategy

**Introduction**

 - What is an Acceptance Test?
 - What are Acceptance Criteria?
 - What is a story?
 - Bring it all together

**Typical Process Overview**

 - The story delivery lifecycle
 - Pick a story
 - Agree acceptance criteria
 - Develop
 - Demonstrate
 - Deliver


###Part 2 - Discussion and alternatives

**Problems acceptance testing tries to fix**

 - Communication barriers
 - Lack of shared memory
 - Lack of collective understanding of requirements
 - Blurring the "what" with the "how"
 - Ambiguous language
 - Lack of structure and direction
 - Team engagement

**Problems acceptance testing can cause**

 - Communication crutch
 - Hand off behaviour
 - Technical over exposure
 - Cargo cult
 - Command and control structures
 - Construct validity
 - Artificial constraints

**Business value** (not yet written)

 - What is "value"?
 - Measuring "value"

**Alternatives to acceptance tests**

 - Don’t write acceptance tests
 - Use a ports and adapters architecture
 - Don’t specify
 - Measure don’t agree
 - Log don’t specify

**How design can influence testing**

 - Sample application
 - Coupled architecture
 - Decoupled architecture using ports and adapters
 - Testing end-to-end (system tests)
 - Benefits using ports and adapters
 - Disadvantages using ports and adapters

**Common pitfalls** (not yet written)

 - Features hit production that the customer didn’t want
 - Users describe solutions not problems
 - Users can’t tell how the system is supposed to behave
 - Users can’t tell if feature x is already implemented
 - Tests repeat themselves
 - The acceptance test suite takes forever to run
 - Intermittent failures in tests

**Q&A** (not yet written)

 - What do we mean when we say “acceptance testing”
 - How do I manage large numbers of acceptance tests?
 - How do you map acceptance tests to stories in say JIRA?
 - How does applying acceptance testing techniques help us focus on reducing complexity?
 - When would you not write stories? Acceptance criteria?
 - How does agile acceptance testing differ from conventional style UAT?
 - What are some other testing strategies? How does acceptance testing fit in?
 - How do you layer various types of testing to maximise benefit?
 - How does exception testing fit with unit and end-to-end tests?
 - Aren’t acceptance tests slow with high maintenance costs?
 - What’s the best way to leverage CI servers like TeamCity and Jenkins?
 - Where does BDD fit in?
 - Can I run acceptance tests in parallel?
 - How can I run acceptance tests which exercise business processes than span multiple business days?
 - How should I setup and tear down data?



###Part 3 - Specification testing frameworks

**The frameworks** (not yet written)

 - Concordion/.NET
 - Yatspec
 - Fit